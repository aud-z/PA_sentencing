{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this jupyter notebook is essentially the same as the \"recidivism-check\" notebook, just cleaned up a bit (hence the name)\n",
    "#import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "#get the folder path for this data\n",
    "pa_sentencing_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/t9x4mbrd2qz1gg4gt4t9rttc0000gn/T/ipykernel_24504/4193884409.py:5: DtypeWarning: Columns (5,15,17,29,30,32,48,58,63,67,68,74,75,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  psc_trimmed = pd.read_csv(os.path.join(pa_sentencing_path, \"Project\", \"data\", \"trimmed_w_prs8.csv\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read in the correct data file (need to read in this file because of the additional columns it has)\n",
    "#psc_trimmed = pd.read_csv(os.path.join(pa_sentencing_path, \"Project\", \"data\", \"PSC_data_trimmed_v1.csv\"))\n",
    "\n",
    "#read in trimmed version WITH 8th edition PRS score implementation\n",
    "psc_trimmed = pd.read_csv(os.path.join(pa_sentencing_path, \"Project\", \"data\", \"trimmed_w_prs8.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Method to Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask import dataframe as dd\n",
    "# from dask.distributed import Client\n",
    "\n",
    "# pa_sentencing_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "# #read in the correct data file (need to read in this file because of the additional columns it has)\n",
    "# psc_trimmed = dd.read_csv(os.path.join(pa_sentencing_path, \"Project\", \"data\", \"PSC_data_trimmed_v1.csv\"),\n",
    "# dtype={'GUILTY_NO_PENALTY': 'object',\n",
    "#        'INC_RELATEDOTN': 'object',\n",
    "#        'INC_RELATIONSHIP': 'object',\n",
    "#        'OFN_COUNT': 'object',\n",
    "#        'OFN_LIFE_DEATH': 'object',\n",
    "#        'PRS': 'object',\n",
    "#        'PRS_LAPSING': 'object',\n",
    "#        'PRS_MANUAL': 'object',\n",
    "#        'PRS_NONLAPSING': 'object',\n",
    "#        'REASON_ONE': 'object',\n",
    "#        'REASON_THREE': 'object',\n",
    "#        'REASON_TWO': 'object',\n",
    "#        'SGR_LVL': 'object',\n",
    "#        'STAT_MIN': 'object'},low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the original loaded data to a working data frame to use and compare with later\n",
    "#df = df_tbl_db.copy() #if accessing the database\n",
    "\n",
    "df = psc_trimmed.copy() # if accessing the psc_trimmed file directly\n",
    "\n",
    "\n",
    "#change column names to uppercase\n",
    "df.columns = df.columns.str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Potential Issue: There are two separate variables for PRS Score -- so just checking here that they are infact the same \n",
    "print(df.PRS8.value_counts())\n",
    "\n",
    "print(df[\"PRS8.1\"].value_counts())\n",
    "\n",
    "#confirmed (based on below output that these are the same variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the issue with the NEW DOF (id_var == 1468038) \n",
    "# df_test = df.copy()\n",
    "\n",
    "# df_test = df_test[df_test[\"ID_VARIABLE\"] == 1468038]\n",
    "\n",
    "# df_test[[\"JPR_ID\", \"ID_VARIABLE\", \"DOS\", \"DOF\", \"OFN_LABEL\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Data Quality Checks & Data Cleaning\n",
    "    1. Consistency Issues\n",
    "        1. [Combine REVOC and RFEL Categories](#combining-revoc-into-rfel)\n",
    "        2. [Clean Dates & Create New DOF](#get-minimum-value-for-the-dof-across-all-of-the-charges-associated-with-one-jprid)\n",
    "    2. Accuracy Issues\n",
    "        1. [Missing PRS Scores](#clean-missing-prs-score)\n",
    "        1. [Address JP_CC_BUG Issue](#clean-jp-cc-bug)\n",
    "2. [At-Risk Date Calculation](#implement-at-risk-date-calculation-logic)\n",
    "    1. Group Data at JPR_ID Level\n",
    "        1. [Address Mutiple Dates of Sentencing](#multiple-dos-for-one-jprid)\n",
    "        2. [Create Adjusted JP_MIN Value]\n",
    "        3. [Check INC_SANCTION EXISTS](#incsanctionexists-check)\n",
    "    2. Group Data at the ID_VARIABLE, DOS LEVEL\n",
    "    3. Implement At-Risk Date Logic\n",
    "3. Calculate Recidivism\n",
    "    1. [Calculate Next Date of Offense](#populate-next-dof)\n",
    "    2. [Check for Free Time](#check-for-\"free-time\")\n",
    "    3. [Calculate Time to Recidivate & 3-Year and 5-Year Recidivism Variables](#create-time-to-recidivate-and-recidivsm-variables)\n",
    "\n",
    "Note: the links aove may not work well in VSCode but wold work better in Jupyter Notebooks via Anaconda\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Dimensions of the Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the Dimensions of the Original Main.CSV dataset\n",
    "psc_main_fewrows = pd.read_csv(os.path.join(pa_sentencing_path, \"Project\", \"data\", \"Main.csv\"),nrows=10)\n",
    "print(psc_main_fewrows.shape[1],'Total Number of columns  in the original dataset')\n",
    "print(psc_trimmed.shape[0],'Total Rows in the original dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining REVOC into RFEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values before conversion\n",
    "df['PRS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine REVOC and spelling issues PRS categories into RFEL\n",
    "def refl_combine(x):\n",
    "    if x in ['REVOC', 'rfel', 'RFEl','Rfel', 'revoc']:\n",
    "        return('RFEL')\n",
    "    else:\n",
    "        return(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRS'] = df['PRS'].apply(refl_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRS'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that Pandas pd.to_datetime is not messing up the format for dates ending in 2020 in DOS. \n",
    "# Extracting out the last two digits of the DOS string and inspecting that there are no \"2020\" values \n",
    "\n",
    "date_list= list(df['DOS'].astype(\"str\"))\n",
    "year_string = [i[-2:] for i in date_list ]\n",
    "print(set(year_string))   \n",
    "'20' in set(year_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date strings to datetime variable\n",
    "df[['DOF','DOS']] = df[['DOF','DOS']].apply(pd.to_datetime,format=\"%d %b %y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing code \n",
    "# df_test = df.copy()\n",
    "\n",
    "# df_test = df_test[df_test[\"ID_VARIABLE\"] == 1468038]\n",
    "\n",
    "# df_test[[\"JPR_ID\", \"ID_VARIABLE\", \"DOS\", \"DOF\", \"OFN_LABEL\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting out the just the year from the date to be used later \n",
    "df['DOF_YEAR'] = pd.DatetimeIndex(df['DOF']).year\n",
    "df['DOS_YEAR'] = pd.DatetimeIndex(df['DOS']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the range of values for the DOF and DOS variables\n",
    "print(\"The minimum date of offense in the dataset is: {}\".format(df[[\"DOF\"]].min()[0]))\n",
    "print(\"The maximum date of offense in the dataset is: {}\".format(df[[\"DOF\"]].max()[0]))\n",
    "print(\"The minimum date of sentencing in the dataset is: {}\".format(df[[\"DOS\"]].min()[0]))\n",
    "print(\"The maximum date of sentencing in the dataset is: {}\".format(df[[\"DOS\"]].max()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: As shown in the above code chunk, there **isn't** anamolous behavior in the date ranges (i.e. a date in the year 1909 or 2090) for the date of offense (DOF) or date of sentence (DOS) variables -- therefore, an additional date correction was **not** applied in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean DOS > DOF\n",
    "\n",
    "Note: group offense by ID_VAR, JPR_ID, MIN(DOF) to get the first DOF associated for a single JPR_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #count how many values of DOF are missing in the original dataset\n",
    "dof_missing = df[df['DOF'].isnull()]\n",
    "\n",
    "print(\"There are {:,} rows with missing DOFs in the dataset.\".format(len(dof_missing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get **minimum** value for the DOF across all of the charges associated with **one** JPR_ID. \n",
    "\n",
    "Note: This is the procedure because we don't wan't to count a DOF as an instance of recidivism if it occurs BEFORE the date of sentencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at the JPR_ID level we only want ONE DOF because becuase we don't want to take into account DOF's that occur\n",
    "#BEFORE the DOS (associated with the JPR_ID) as an instance of recidivism. -- each JPR_ID should have only ONE DOS\n",
    "\n",
    "#df[\"NEW_DOF\"] = df.groupby([\"JPR_ID\"])[\"DOF\"].transform(\"min\")\n",
    "\n",
    "#here we will group by id_Variable as well as jpr_id\n",
    "df[\"NEW_DOF\"] = df.groupby([\"JPR_ID\", \"ID_VARIABLE\"])[\"DOF\"].transform(\"min\")\n",
    "\n",
    "# df[\"MIN_DOF\"] = df.groupby([\"JPR_ID\"])[\"DOF\"].transform(\"min\")\n",
    "# df[\"MAX_DOF\"] = df.groupby([\"JPR_ID\"])[\"DOF\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing code\n",
    "df_test = df.copy()\n",
    "\n",
    "df_jprid = df_test[df_test[\"JPR_ID\"] == 5499834]\n",
    "df_test = df_test[df_test[\"ID_VARIABLE\"] == 1468038]\n",
    "\n",
    "#here is where the issue seems to occur\n",
    "\n",
    "df_test[[\"JPR_ID\", \"ID_VARIABLE\", \"DOS\", \"DOF\", \"NEW_DOF\",\"OFN_LABEL\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still part of the testing of the code\n",
    "#df_jprid[[\"JPR_ID\", \"ID_VARIABLE\", \"DOS\", \"DOF\", \"MIN_DOF\", \"MAX_DOF\", \"NEW_DOF\",\"OFN_LABEL\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the code\n",
    "# 'test_date = \"2014-08-18\"\n",
    "# print(type(test_date))\n",
    "\n",
    "# test_date_conversion = pd.to_datetime(test_date) #, format=\"%d %b %y\")\n",
    "# print(test_date_conversion, type(test_date_conversion))\n",
    "\n",
    "# test_date_conversion.min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()[[\"JPR_ID\", \"DOF\", \"NEW_DOF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dof_missing = df[df['NEW_DOF'].isnull()]\n",
    "\n",
    "percent_missing = len(dof_missing)/len(df)\n",
    "print(\"After cleaning, there are {:,} ({:%}) rows with missing DOFs in the dataset.\".format(len(dof_missing), percent_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: Subset the data to just include those rows where NEW_DOF <= DOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the sentencing \n",
    "before_length = len(df)\n",
    "df = df[df.NEW_DOF <= df.DOS] #should this be <= ?\n",
    "after_length = len(df)\n",
    "\n",
    "print(\"Before DOF <= DOS correction there were {:,} rows and after cleaning there were {:,} rows. A change of {:,}.\".format(before_length, after_length, before_length - after_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Missing PRS Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_length = len(df)\n",
    "#subset to just the id variables with a PRS score missing\n",
    "id_varswith_prsmissing= set(df[df.PRS.isnull()].ID_VARIABLE)\n",
    "\n",
    "#remove id vars with missing PRS\n",
    "df_prs_notaffected = df[~df.ID_VARIABLE.isin(id_varswith_prsmissing)]\n",
    "\n",
    "#reassign to working dataframe\n",
    "df = df_prs_notaffected \n",
    "\n",
    "after_length = len(df)\n",
    "print(\"Before PRS correction there were {:,} rows and after cleaning there were {:,} rows. A change of {:,} rows and {} people.\".format(before_length, after_length, before_length - after_length, len(id_varswith_prsmissing)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Missing PRS8 (8th Edition Sentencing Guidelines) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_length = len(df)\n",
    "#subset to just the id variables with a PRS score missing\n",
    "id_varswith_prs8missing= set(df[df.PRS8.isnull()].ID_VARIABLE)\n",
    "\n",
    "#remove id vars with missing PRS\n",
    "df_prs8_notaffected = df[~df.ID_VARIABLE.isin(id_varswith_prs8missing)]\n",
    "\n",
    "#reassign to working dataframe\n",
    "df = df_prs8_notaffected \n",
    "\n",
    "after_length = len(df)\n",
    "print(\"Before PRS8 correction there were {:,} rows and after cleaning there were {:,} rows. A change of {:,} rows and {} people.\".format(before_length, after_length, before_length - after_length, len(id_varswith_prs8missing)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean JP CC Bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps followed in cleaning JP_CC Bug\n",
    "1. It is evident that there are JPR_ID's with DOS from 2016 to 2019 that were impacted by the JP_CC_BUG \n",
    "2. The first step was to extract out the ID variables that were impacted out by the bug. \n",
    "3. next we removed the Judicial proceedings of these JPR_ID's where the DOS is in 2017,2018, and 2019. However, the JPR_ID's associated with the first occurence of the JP_CC Bug is kept- In other words, the JPR_ID's where the DOS was in 2016 is kept. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirming the years that impacted the JP_CC_BUG\n",
    "set(df[df.JP_CC_BUG=='Y'].DOS_YEAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the id variables with jp_bug\n",
    "id_varswith_jpbug= set(df[df.JP_CC_BUG=='Y'].ID_VARIABLE) #pull out both id_variable and DOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning all the rows associated with the jp bugs to a seperate dataframe \n",
    "df_with_jpbug=  df[df.ID_VARIABLE.isin(id_varswith_jpbug)]  #want to remove the charges that come after the DOS associated with the JP_CC_BUG row (want to eliminate the problemative date of sentencing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the JPR'IDS's that have DOS in 2017,2018 and 2019\n",
    "df_jp_bug_cleaned = df_with_jpbug[df.DOS_YEAR<2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating the rows associated with id_vars in the original dataframe that is not associated with the bug\n",
    "df_jpbug_notaffected = df[~df.ID_VARIABLE.isin(id_varswith_jpbug)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoining the rows affected by the JP_CC_bug after cleaning them to the rows not affected by the bug\n",
    "df_cleaned_1 = pd.concat([df_jpbug_notaffected,df_jp_bug_cleaned])  #new working df\n",
    "\n",
    "df = df_cleaned_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_length = len(df)\n",
    "\n",
    "print(\"After the JP_CC_BUG correction there are {:,} rows. \".format(after_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement At Risk Date Calculation Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 1:** \n",
    "#### 1. Fix issues where one JPR_ID has more than one date of sentence NEW_DOS = (min(dos)) and JPMIN =  lastest (most recent jp-min) -- which is associated with the max(dos) -- the most recent date of sentence & \n",
    "#### 2. make sure that the row that we subset at includes a inc_sanction_exists == yes if at LEAST one of the charges in the list is equal to Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple DOS for one JPR_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_vals = df.groupby([\"JPR_ID\"])['DOS'].agg(lambda x: set(x)) #> 1 #how many rows have two UNIQUE DOS for the same JPR_ID (set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dos = df.groupby([\"JPR_ID\"])['DOS'].agg(lambda x: len(set(x))) #how many dates of sentencing does each jpr_id have?\n",
    "#num_dos[2847193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more_than_one_dos.reset_index()\n",
    "more_than_one_dos = list(num_dos[num_dos > 1].index)\n",
    "\n",
    "print(\"There are {:,} JPR_IDS in the dataset with more than one date of sentence.\".format(len(more_than_one_dos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of multiple dates of sentencing\n",
    "#df.loc[df[\"JPR_ID\"] == 662328][[\"JPR_ID\", \"ID_VARIABLE\", \"DOS\", \"OFN_LABEL\", \"PRS\", \"JP_MIN\", \"ADJ_JPMIN\", \"MS_SENTJP\", \"INC_SANCTION_EXISTS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the data JUST to those ID's in the more_than_one dos bucket\n",
    "only_one_dos = df.loc[~df[\"JPR_ID\"].isin(more_than_one_dos)]\n",
    "more_than_one_dos_df = df.loc[df[\"JPR_ID\"].isin(more_than_one_dos)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_one_dos_df = more_than_one_dos_df.sort_values([\"JPR_ID\", \"DOS\"]) #sort by jpr_id AND DOS\n",
    "\n",
    "#more_than_one_dos_df.head()[[\"JPR_ID\", \"DOS\", \"OFN_LABEL\", \"PRS\", \"JP_MIN\", \"ADJ_JPMIN\", \"MS_SENTJP\", \"INC_SANCTION_EXISTS\"]]\n",
    "more_than_one_dos_df.head()[[\"JPR_ID\", \"DOS\", \"OFN_LABEL\", \"PRS\", \"JP_MIN\", \"MS_SENTJP\", \"INC_SANCTION_EXISTS\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column with the NEW_DOS value\n",
    "more_than_one_dos_df[\"MAX_DOS\"] = more_than_one_dos_df.groupby(\"JPR_ID\")[\"DOS\"].transform(\"max\") #take the latest date of sentencing\n",
    "more_than_one_dos_df[\"MIN_DOS\"] = more_than_one_dos_df.groupby(\"JPR_ID\")[\"DOS\"].transform(\"min\") #take the latest date of sentencing\n",
    "\n",
    "# #create a new time served column\n",
    "more_than_one_dos_df[\"TIME_SERVED\"] = more_than_one_dos_df[\"MAX_DOS\"] - more_than_one_dos_df[\"MIN_DOS\"]\n",
    "more_than_one_dos_df[\"TIME_SERVED\"] = more_than_one_dos_df[\"TIME_SERVED\"].dt.days\n",
    "\n",
    "#finds the JP_MIN associated with the latest DOS (because the data is already sorted by JPR_ID and DOS)\n",
    "more_than_one_dos_df[\"LATEST_JPMIN\"] = more_than_one_dos_df.groupby(\"JPR_ID\")[\"JP_MIN\"].transform(\"last\")\n",
    "\n",
    "#calculate an adjusted JP_MIN from the logic provided by Miranda\n",
    "more_than_one_dos_df['ADJ_JPMIN'] = more_than_one_dos_df[\"LATEST_JPMIN\"] - more_than_one_dos_df[\"TIME_SERVED\"]\n",
    "\n",
    "# # more_than_one_dos_df[:20][[\"JPR_ID\", \"DOS\", \"OFN_LABEL\", \"PRS\", \"JP_MIN\", \"ADJ_JPMIN\", \"INC_SANCTION_EXISTS\", \"NEW_DOS\"]]\n",
    "more_than_one_dos_df[:20][[\"JPR_ID\", \"ID_VARIABLE\", \"DOS\", \"OFN_LABEL\", \"MIN_DOS\", \"JP_MIN\", \"MAX_DOS\", \"TIME_SERVED\", \"LATEST_JPMIN\", \"ADJ_JPMIN\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the data back together\n",
    "\n",
    "df_combo_dos = pd.concat([only_one_dos, more_than_one_dos_df])\n",
    "\n",
    "df = df_combo_dos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 2:** Create a New JP_MIN variable that takes the Max(JP_MIN) for a given JPR_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix Issues with the missing JP_MIN\n",
    "# num_missing_jp_min = len(df.loc[pd.isna(df[\"JP_MIN\"])]) #[[\"JPR_ID\", \"JP_MIN\"]]\n",
    "# print(\"There are {:,} entries in the dataset missing a JP_MIN value.\".format(num_missing_jp_min))\n",
    "\n",
    "# df[\"ADJ_JPMIN\"] = df.groupby([\"JPR_ID\"])[\"JP_MIN\"].transform(\"max\")\n",
    "\n",
    "# num_missing_jp_min = len(df.loc[pd.isna(df[\"ADJ_JPMIN\"])]) #[[\"JPR_ID\", \"JP_MIN\"]]\n",
    "# print(\"There are {:,} entries in the dataset missing a  ADJ_JPMIN value.\".format(num_missing_jp_min))\n",
    "\n",
    "#when should we calculated a consolidated JP_MIN? before or after grouping at the id_variable, dos level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the results\n",
    "\n",
    "#impute missing values for the ADJ_JPMIN values that are currently null -- if there are multiple JP_MINS for an instance that DOESNT have multiple sentencing dates, \n",
    "# just take the maximum value of the JP_MIN available and set it equal to the adj_jpmin value\n",
    "\n",
    "df.loc[df[\"ADJ_JPMIN\"].isnull(), \"ADJ_JPMIN\"] =  df.groupby([\"JPR_ID\"])[\"JP_MIN\"].transform(\"max\")\n",
    "\n",
    "\n",
    "#df.sort_values([\"JPR_ID\"])[:20][[\"JPR_ID\", \"DOS\", \"MIN_DOS\", \"JP_MIN\", \"MAX_DOS\", \"TIME_SERVED\", \"LATEST_JPMIN\", \"ADJ_JPMIN\"]]\n",
    "\n",
    "\n",
    "#[[\"JPR_ID\", \"DOS\", \"MIN_DOS\", \"JP_MIN\", \"MAX_DOS\", \"TIME_SERVED\", \"LATEST_JPMIN\", \"ADJ_JPMIN\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"JPR_ID\", \"DOS\", \"MIN_DOS\", \"JP_MIN\", \"MAX_DOS\", \"TIME_SERVED\", \"LATEST_JPMIN\", \"ADJ_JPMIN\"]]\n",
    "\n",
    "df.loc[df[\"MAX_DOS\"].isnull()][[\"JPR_ID\", \"DOS\", \"MIN_DOS\", \"JP_MIN\", \"MAX_DOS\", \"TIME_SERVED\", \"LATEST_JPMIN\", \"ADJ_JPMIN\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INC_SANCTION_EXISTS Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out how many different values INC_SANCTION_EXISTS takes on for each JPR_ID\n",
    "#if at least 1 charge is = Y (JPR_ID might have Yes and NO) -- then inc_sanction_exists for the ENTIRE JPR_ID should be \"Y\"\n",
    "\n",
    "num_inc_sanc_vals = df.groupby([\"JPR_ID\"])['INC_SANCTION_EXISTS'].agg(lambda x: len(set(x))) #INC_SANCTION_EXISTS values does each jpr_id have?\n",
    "\n",
    "#give back the list of JPR_IDs that have more than one inc_sanction_exists value\n",
    "more_than_one_inc_sanc = list(num_inc_sanc_vals[num_inc_sanc_vals > 1].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more_than_one_inc_sanc\n",
    "\n",
    "#df.loc[df[\"JPR_ID\"] == 2286][[\"JPR_ID\", \"DOS\", \"INC_SANCTION_EXISTS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one_inc_sanc = df.loc[~df[\"JPR_ID\"].isin(more_than_one_inc_sanc)]\n",
    "more_than_one_inc_sanc_df = df.loc[df[\"JPR_ID\"].isin(more_than_one_inc_sanc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more_than_one_inc_sanc_df[[\"JPR_ID\", \"DOS\", \"INC_SANCTION_EXISTS\"]]\n",
    "\n",
    "more_than_one_inc_sanc_df[\"NEW_INC_SANCTION_EXISTS\"] = \"Y\"\n",
    "more_than_one_inc_sanc_df[[\"JPR_ID\", \"DOS\", \"INC_SANCTION_EXISTS\", \"NEW_INC_SANCTION_EXISTS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_sanc_combined = pd.concat([only_one_inc_sanc, more_than_one_inc_sanc_df])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_sanc_combined[[\"JPR_ID\", \"DOS\", \"INC_SANCTION_EXISTS\", \"NEW_INC_SANCTION_EXISTS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = inc_sanc_combined\n",
    "\n",
    "df.loc[df[\"NEW_INC_SANCTION_EXISTS\"].isnull(), \"NEW_INC_SANCTION_EXISTS\"] =  df[\"INC_SANCTION_EXISTS\"]\n",
    "\n",
    "df[[\"JPR_ID\", \"DOS\", \"INC_SANCTION_EXISTS\", \"NEW_INC_SANCTION_EXISTS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Note: As shown below, there are no entries before we collapse at the ID_VAR, DOS-LEVEL with the adj_jpmin & new_inc_sanction_exists mismatch\")\n",
    "df.loc[(df[\"ADJ_JPMIN\"] > 0) & (df[\"NEW_INC_SANCTION_EXISTS\"] == \"N\")][[\"ID_VARIABLE\", \"DOS\", \"NEW_DOF\", \"NEW_INC_SANCTION_EXISTS\", \"ADJ_JPMIN\"]] #, \"OFN_LIFE_DEATH\", \"JP_LIFE_DEATH\"]] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MAX PRS8 Score at the JPR_ID level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRS8'] = df.groupby([\"JPR_ID\"])['PRS8'].transform(\"max\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder: Need to Add Flags for the Following categories (during the collapsing process)\n",
    "* Crimes of violence\n",
    "    * There is some public opinion that the existing definition in the guidelines may be too narrow. The public prefers a broader consideration of violent offenses (e.g.  Any violence against a person)\n",
    "        * Not sure if there's an easy way to identify these\n",
    "* Sex offenses\n",
    "    * There are 3 tiers of sex offenses\n",
    "        * Look at recidivism rates for the general categorization and for each of the three tiers\n",
    "* Firearms (VUFA - violation of the uniform fire offense types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapse the data at the ID_VARIABLE, DOS-LEVEL \n",
    "\n",
    " *** Changed this to be at the id_variable, new-dos level (on 4/11/22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a NEW_DOS Variable To Account for Individuals with Multiple Dates of Sentencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create the new date of sentencing variable 4/11/22\n",
    "# df_collapsed = df.copy()\n",
    "\n",
    "# df_collapsed.loc[df_collapsed[\"MAX_DOS\"].notna(), \"NEW_DOS\"] = df_collapsed[\"MAX_DOS\"]\n",
    "\n",
    "# df_collapsed.loc[df_collapsed[\"MAX_DOS\"].isnull(), \"NEW_DOS\"] = df_collapsed[\"DOS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collapsed = df.copy()\n",
    "\n",
    "#do people have different PRS scores for the same JPR_ID?\n",
    "\n",
    "# #get the max values of the OGS and JP_MIN values -- possibly further adjustments need to be at this level\n",
    "df_collapsed['OGS'] = df_collapsed.groupby([\"ID_VARIABLE\", \"DOS\"])[\"OGS\"].transform(max)\n",
    "\n",
    "#for the same id_variable, DOS pairing, get the \"MIN\" NEXT_DOF\n",
    "df_collapsed['NEW_DOF'] = df_collapsed.groupby([\"ID_VARIABLE\", \"DOS\"])[\"NEW_DOF\"].transform(min)\n",
    "\n",
    "#added this in on 4/11/22\n",
    "df_collapsed[\"ADJ_JPMIN\"] = df_collapsed.groupby([\"ID_VARIABLE\", \"DOS\"])[\"ADJ_JPMIN\"].transform(max)\n",
    "\n",
    "#get the max PRS8 score at the id_variable, date of sentencing level\n",
    "df_collapsed[\"PRS8\"] = df_collapsed.groupby([\"ID_VARIABLE\", \"DOS\"])[\"PRS8\"].transform(max)\n",
    "\n",
    "#combine the offense type flags into one string\n",
    "\n",
    "#collapse data to be at the id variable, DOS level (need to ungroup the data for the at_risk date calculation to work)\n",
    "df_collapsed = df_collapsed.copy().groupby([\"ID_VARIABLE\", \"DOS\"]).first().reset_index()\n",
    "\n",
    "#inspect the results\n",
    "df_collapsed[[\"ID_VARIABLE\", \"DOS\", \"NEW_DOF\", \"NEW_INC_SANCTION_EXISTS\", \"ADJ_JPMIN\"]] #, \"OFN_LIFE_DEATH\", \"JP_LIFE_DEATH\"]] \n",
    "\n",
    "\n",
    "#deal with the INC_SANCTION_EXISTS PART OF THIS -- don't only keep (keep the one with Y and not no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK INC_SANCTION_EXIST ADJ_JPMIN > 0 and INC_SANCTION_EXISTS = 'N'\n",
    "\n",
    "#df[(df['col1'] >= 1) & (df['col1'] <=1 )]\n",
    "#df_collapsed.loc[(df_collapsed[\"ADJ_JPMIN\"] > 0) & (df_collapsed[\"NEW_INC_SANCTION_EXISTS\"] == \"N\")][[\"ID_VARIABLE\", \"DOS\", \"NEW_DOF\", \"NEW_INC_SANCTION_EXISTS\", \"ADJ_JPMIN\"]] #, \"OFN_LIFE_DEATH\", \"JP_LIFE_DEATH\"]] \n",
    "\n",
    "\n",
    "#df.loc[df['TeamID']==12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 3:** Calculate the AT_RISK_DT using the following logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_at_risk_date(row):\n",
    "    #need to account for REALLY large JP_MIN values\n",
    "    \n",
    "    # Because of this error message OverflowError: Python int too large to convert to C long\n",
    "    # 25 is more years than we have in our data, so their at_risk date also get set to some value far in the future\n",
    "    upper_limit = 25.0 * 365.0\n",
    "    \n",
    "    num_days_in_month = 30.0\n",
    "    \n",
    "    #if offense has a life or death flag, set their at_risk_date abritarily large\n",
    "    if row['OFN_LIFE_DEATH'] == \"Y\":\n",
    "        at_risk_date = pd.to_datetime('2035-12-31')\n",
    "    \n",
    "    if row['JP_LIFE_DEATH'] == \"Y\":\n",
    "        at_risk_date = pd.to_datetime('2035-12-31')\n",
    "\n",
    "    #if they were not incarcerated, then their at risk date is just their date of offense\n",
    "    if row[\"NEW_INC_SANCTION_EXISTS\"] == \"N\":\n",
    "        at_risk_date = row['DOS'] #(was previously DOF but should be DOS)\n",
    "    \n",
    "    #if they were incarcerated, look at the below logic to determine their at-risk date\n",
    "    else:\n",
    "\n",
    "        if row[\"ADJ_JPMIN\"] < upper_limit:\n",
    "\n",
    "            if row[\"NEW_INC_SANCTION_EXISTS\"] == \"Y\" and pd.notna(row['ADJ_JPMIN']):\n",
    "                at_risk_date = row['DOS'] + pd.Timedelta(days = row['ADJ_JPMIN'])\n",
    "            \n",
    "            elif row[\"NEW_INC_SANCTION_EXISTS\"] == \"Y\" and pd.notna(row['INCMIN']):\n",
    "                at_risk_date = row['DOS'] + pd.Timedelta(days = row['INCMIN'] * num_days_in_month)\n",
    "\n",
    "            else:\n",
    "                at_risk_date = row['INC_END']\n",
    "\n",
    "        else:\n",
    "            at_risk_date = pd.to_datetime('2035-12-31')\n",
    "\n",
    "    #address the special case where there are multiple dates of sentencing\n",
    "    # if pd.notna(row['MAX_DOS']) and pd.notna(row['ADJ_JPMIN']):\n",
    "    #     at_risk_date = row['MAX_DOS'] + pd.Timedelta(days = row['ADJ_JPMIN'])\n",
    "\n",
    "    \n",
    "    return at_risk_date\n",
    "\n",
    "\n",
    "# df[\"AT_RISK_DT\"] = np.where(\n",
    "#     df['INC_SANCTION_EXISTS'] == \"Y\" and pd.notna(df['JP_MIN']), 1, 0)\n",
    "\n",
    "# test = df[:2000]\n",
    "# #apply the function to the data (row by row)\n",
    "# test[\"AT_RISK_DT\"] = test.apply(create_at_risk_date, axis = 1)\n",
    "\n",
    "#  #adjust so that the times do not include minutes and seconds\n",
    "# test[\"AT_RISK_DT\"] = pd.to_datetime(test[\"AT_RISK_DT\"]).dt.date\n",
    "\n",
    "# # #inspect the results\n",
    "# test[['ID_VARIABLE', 'JPR_ID',\"JP_MIN\", \"INCMIN\", \"INC_END\", \"ADJ_JPMIN\", \"INC_SANCTION_EXISTS\", \"DOS\", \"NEW_DOF\", \"AT_RISK_DT\"]]\n",
    "\n",
    "#test = df[:2000]\n",
    "#apply the function to the data (row by row)\n",
    "df_collapsed[\"AT_RISK_DT\"] = df_collapsed.apply(create_at_risk_date, axis = 1)\n",
    "\n",
    " #adjust so that the times do not include minutes and seconds\n",
    "df_collapsed[\"AT_RISK_DT\"] = pd.to_datetime(df_collapsed[\"AT_RISK_DT\"]).dt.date\n",
    "\n",
    "# #inspect the results\n",
    "df_collapsed[['ID_VARIABLE', 'JPR_ID',\"JP_MIN\", \"INCMIN\", \"INC_END\", \"ADJ_JPMIN\", \"INC_SANCTION_EXISTS\", \"NEW_DOS\", \"NEW_DOF\", \"AT_RISK_DT\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the above at_risk_date calculation code, there is an \"upper_limit\" because the largest JP_MIN value is 230,000+ days, which is the equivalent of about 631 years. This person would not recidivate in our dataset and Python throws a \"OverflowError: Python int too large to convert to C long\" for these individuals. So, in order to allow the code to run, those with jp_min values equivalent to more days than we have data for, will just get an at-risk date very far into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #correct the at_risk_dt calculation for some rows:\n",
    "df_collapsed.loc[(~(df_collapsed[\"ADJ_JPMIN\"]).isnull()) & (~(df_collapsed['MAX_DOS'].isnull())), \"AT_RISK_DT\"] = df_collapsed['MAX_DOS']  + pd.to_timedelta(df_collapsed['ADJ_JPMIN'], unit='d')\n",
    "\n",
    "#will another line here work to resolve this issue?\n",
    "df_subset_mult = df_collapsed.loc[(~(df_collapsed[\"ADJ_JPMIN\"]).isnull()) & (~(df_collapsed['MAX_DOS'].isnull()))]  #, \"AT_RISK_DT\"] # = pd.to_datetime(df_collapsed[\"AT_RISK_DT\"])\n",
    "df_subset_rest = df_collapsed.loc[(~(df_collapsed[\"ADJ_JPMIN\"]).isnull()) & ((df_collapsed['MAX_DOS'].isnull()))]  #, \"AT_RISK_DT\"] # = pd.to_datetime(df_collapsed[\"AT_RISK_DT\"])\n",
    "\n",
    "\n",
    "\n",
    "# # #for some reason, the above gives dates (although correct) in the following format:1556150400000000000)\n",
    "#df_collapsed.loc[(~(df_collapsed[\"ADJ_JPMIN\"]).isnull()) & (~(df_collapsed['MAX_DOS'].isnull()))].head()[['ID_VARIABLE', 'JPR_ID', \"MAX_DOS\",\"ADJ_JPMIN\", \"NEW_DOF\", \"AT_RISK_DT\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_mult[['ID_VARIABLE', 'JPR_ID', \"MAX_DOS\",\"ADJ_JPMIN\", \"NEW_DOF\", \"AT_RISK_DT\"]]\n",
    "\n",
    "df_subset_mult[\"AT_RISK_DT\"] = pd.to_datetime(df_subset_mult[\"AT_RISK_DT\"])\n",
    "\n",
    "df_subset_mult[['ID_VARIABLE', 'JPR_ID', \"MAX_DOS\",\"ADJ_JPMIN\", \"NEW_DOF\", \"AT_RISK_DT\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the dataframes back together\n",
    "df_collapsed = pd.concat([df_subset_mult, df_subset_rest])\n",
    "\n",
    "df_collapsed.loc[(~(df_collapsed[\"ADJ_JPMIN\"]).isnull()) & (~(df_collapsed['MAX_DOS'].isnull()))].head()[['ID_VARIABLE', 'JPR_ID', \"MAX_DOS\",\"ADJ_JPMIN\", \"NEW_DOF\", \"AT_RISK_DT\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OverflowError: Python int too large to convert to C long\n",
    "\n",
    "largest_jpmin =  df_collapsed[\"ADJ_JPMIN\"].max()\n",
    "largest_jpmin_in_years = largest_jpmin/365.0\n",
    "print(\"The largest JP_MIN value is {:,} days, which is {} years. This causes Python to throw the following error: OverflowError: Python int too large to convert to C long.\".format(largest_jpmin, largest_jpmin_in_years))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Next DOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data\n",
    "df_collapsed = df_collapsed.sort_values(by = [\"ID_VARIABLE\", \"NEW_DOF\"])\n",
    "\n",
    "#shift the data up by one to create the new vaariable \"NEXT_DOF\"\n",
    "df_collapsed['NEXT_DOF'] = df_collapsed.groupby(['ID_VARIABLE'])['NEW_DOF'].shift(-1).dt.date\n",
    "\n",
    "df_collapsed[:20][[\"ID_VARIABLE\", \"JPR_ID\", \"NEW_DOS\", \"NEW_DOF\", \"NEXT_DOF\", \"AT_RISK_DT\", \"INC_SANCTION_EXISTS\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for \"Free Time\" \n",
    "(i.e.: Do we have enough data for an individual to see if they recidivated in 3 years or not?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procedure Below:**\n",
    "1. Subset just to those whose at_risk date < max DOS df[[\"DOS\"]].max()\n",
    "2. Then, we also want to remove those whose last next_dof is null and whose last dof > 2017\n",
    "3. Essentially, we want to subset (whatever grouping variable we're using) to just those entries where next_dof is null and FOR THIS SAME ROW, if the dof >= pd.todatetime(\"2017-01-01\") -- remove these entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset to those whose at_risk_date < the largest sentencing date that we have\n",
    "\n",
    "before_length = len(df_collapsed)\n",
    "\n",
    "#what is the maximum sentence date?\n",
    "last_day = pd.to_datetime(df_collapsed[[\"DOS\"]].max())[0]  \n",
    "df_collapsed = df_collapsed[df_collapsed[\"AT_RISK_DT\"] <= last_day]\n",
    "\n",
    "after_length = len(df_collapsed) \n",
    "\n",
    "print(\"There are {:,} id_var, dos combos where the at risk date is after the last date of sentence available.\".format(before_length - after_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I calculate a \"LAST_DOF\" variable, which will then be used to subset the data to only those whose latest offense was before 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collapsed[\"LAST_DOF\"] = df_collapsed.loc[df_collapsed[\"NEXT_DOF\"].isnull(), \"NEW_DOF\"]\n",
    "\n",
    "df_collapsed[[\"ID_VARIABLE\", \"DOS\", \"NEW_DOF\", \"NEXT_DOF\", \"LAST_DOF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the data to only those whose last_dof is before 2017\n",
    "#before_length = len(df_collapsed)\n",
    "\n",
    "last_day = pd.to_datetime(\"2017-01-01\") \n",
    "\n",
    "#subset the dataset to either where the LAST_DOF is null OR LAST_DOF < last_day\n",
    "df_collapsed = df_collapsed.loc[(df_collapsed[\"LAST_DOF\"].isnull()) | (df_collapsed[\"LAST_DOF\"] < last_day)]\n",
    "\n",
    "# after_length = len(df_collapsed) \n",
    "# print(\"There are {:,} id_var, dos combos whose's last dof is not in scope.\".format(before_length - after_length))\n",
    "\n",
    "df_collapsed[[\"ID_VARIABLE\", \"DOS\", \"NEW_DOF\", \"NEXT_DOF\", \"LAST_DOF\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE TIME TO RECIDIVATE AND RECIDIVSM VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract the next_dof and at_risk_dt variables  -- update this \n",
    "df_collapsed['TIME_TO_RECIDIVATE'] = pd.to_datetime(df_collapsed['NEXT_DOF']) - pd.to_datetime(df_collapsed['AT_RISK_DT'])#update to this level \n",
    "    \n",
    "#update the time to recidivate column to JUST be the number of days as an integer/float\n",
    "df_collapsed['TIME_TO_RECIDIVATE'] = df_collapsed['TIME_TO_RECIDIVATE'].dt.days\n",
    "\n",
    "df_collapsed[[\"ID_VARIABLE\", \"DOS\", \"NEW_DOF\", \"NEXT_DOF\", \"TIME_TO_RECIDIVATE\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of days in  years\n",
    "three_years_in_days = float(3) * 365.0  \n",
    "five_years_in_days = float(5) * 365.0  \n",
    "\n",
    "#ID_VARIABLE, DOS-LEVEL RECIDIVISM -- does not count times where the next_dof < at_risk_dt as instances of recidivism\n",
    "\n",
    "df_collapsed[\"RECIDIVISM_3Y\"] = np.where(\n",
    "    (df_collapsed['TIME_TO_RECIDIVATE'] > 0) & (df_collapsed['TIME_TO_RECIDIVATE'] <= three_years_in_days), 1, 0)\n",
    "\n",
    "df_collapsed[\"RECIDIVISM_5Y\"] = np.where(\n",
    "    (df_collapsed['TIME_TO_RECIDIVATE'] > 0) & (df_collapsed['TIME_TO_RECIDIVATE'] <= five_years_in_days), 1, 0)\n",
    "\n",
    "df_collapsed[[\"ID_VARIABLE\", \"DOS\", \"ADJ_JPMIN\", \"NEW_DOF\", \"NEXT_DOF\", \"AT_RISK_DT\", \"TIME_TO_RECIDIVATE\", \"RECIDIVISM_3Y\", \"RECIDIVISM_5Y\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export The Results to CSV \n",
    "(PA_SENTENCING/Project/data/recidivism_dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new date of sentence variable\n",
    "\n",
    "#if max_dos is null, then there is only one DOS associated with a given JPR_ID OTHERWISE, the new_dos becomes the max dos (meaning there were multiple dates of sentence associated with that JPR_ID)\n",
    "\n",
    "df_collapsed.loc[df_collapsed[\"MAX_DOS\"].notna(), \"NEW_DOS\"] = df_collapsed[\"MAX_DOS\"]\n",
    "\n",
    "df_collapsed.loc[df_collapsed[\"MAX_DOS\"].isnull(), \"NEW_DOS\"] = df_collapsed[\"DOS\"]\n",
    "\n",
    "df_collapsed[[\"MAX_DOS\",\"DOS\", \"NEW_DOS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the Results to a CSV\n",
    "#subset the dataset before exporting it\n",
    "df_collapsed_subset = df_collapsed[[\"ID_VARIABLE\", \"NEW_DOS\", \"NEW_DOF\", \"PRS\", \"PRS8\", \"NEW_INC_SANCTION_EXISTS\", \"ADJ_JPMIN\",\"AT_RISK_DT\", \"COUNTY\", \"NEXT_DOF\", \"TIME_TO_RECIDIVATE\", \"RECIDIVISM_3Y\", \"RECIDIVISM_5Y\", \"OGS\"]]\n",
    "\n",
    "\n",
    "#get the demographics dataset\n",
    "cleaned_demographics = pd.read_csv(os.path.join(pa_sentencing_path, \"Project\", \"data\", \"demographic_dataset.csv\"))\n",
    "\n",
    "#merged the recidivism dataset with the cleaned demographics dataset\n",
    "result = pd.merge(df_collapsed_subset, cleaned_demographics, how=\"left\", on=[\"ID_VARIABLE\", \"ID_VARIABLE\"])\n",
    "\n",
    "\n",
    "# #export the dataframe with the recidivism variables to a new dataframe\n",
    "#output_path = os.path.join(pa_sentencing_path, \"Project\", \"data\", \"recidivism_dataset.csv\")\n",
    "\n",
    "#changes the output path to include a flag for including the PRS score 8 values\n",
    "output_path = os.path.join(pa_sentencing_path, \"Project\", \"data\", \"recidivism_dataset_w_prs8.csv\")\n",
    "\n",
    "\n",
    "\n",
    "result.to_csv(output_path) #export the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check to see what the data looks like\n",
    "test = pd.read_csv(output_path)\n",
    "\n",
    "# print(\"num rows:\", len(test.index))\n",
    "# test.head()\n",
    "\n",
    "print(test.RECIDIVISM_3Y.value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
